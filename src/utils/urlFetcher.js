// src/utils/urlFetcher.js

import axios from "axios";
import { PassThrough } from "stream";
import { processHtmlWithReadability } from "./primaryContextExtender.js";
import { processHtmlWithCheerio } from "./secondaryContextExtender.js";
import {
  isYoutubeUrl,
  youtubeCaptionFetcher,
  youtubeCaptionScraper,
} from "./youtubeCaptionDownloader.js";
import { SKIPPED_EXTENSIONS, SKIPPED_HOSTS } from "./blockedResources.js";
import unshort from "url-unshort";
import { retryWithDelay } from "./retryMechanism.js";
import config from "../config/index.js";
import logger from "../utils/logger.js";

// Main URL fetching function
export async function fetchUrlContents(urls) {
  return Promise.all(urls.map(fetchUrlContent));
}

// fetching each URL one by one
async function fetchUrlContent(url) {
  try {
    logger.info(`ORIGINAL URL: ${url}`);

    // Check if the original URL has a file extension or is blocklisted
    if (skipUrlByFileExtension(url) || skipUrlByHost(url)) return ""; // Skip processing and return an empty string

    // Resolve the target URL if it's a short link or involves redirection
    const targetUrl = await shortUrlExpander(url);

    logger.info(`TARGET URL: ${targetUrl}`);

    // Check if the original URL also has a file extension or is blocklisted
    if (skipUrlByFileExtension(targetUrl) || skipUrlByHost(targetUrl))
      return ""; // Skip processing and return an empty string

    if (isYoutubeUrl(targetUrl)) {
      // const subtitleText = await youtubeCaptionFetcher(targetUrl);
      const subtitleText = await youtubeCaptionScraper(targetUrl);
      console.log(subtitleText); // This will print a long string of all subtitle text.

      // Generated by Downloading youtube video's caption, summarizable is true
      return { content: subtitleText, summarizable: true, src: "youtube" };
    }

    // Fetch the full content using a GET request (with proxy fallback)
    const responseData = await fetchWithFallback(targetUrl);

    return await processHtml(responseData, targetUrl);
  } catch (error) {
    return { content: "", summarizable: false, src: null };
  }
}

// Create an instance of url-unshort
const uu = unshort();

// Add ift.tt as another supported url-shortener service
uu.add("ift.tt", {
  match: /^https?:\/\/ift\.tt\/.+/, // regex for ift.tt URLs
});

// Function to Un-Shorten the URLs with the help of url-unshort library
// list of supported url-shortener services can be found at https://github.com/nodeca/url-unshort/blob/master/domains.yml
export async function shortUrlExpander(url) {
  return retryWithDelay(async () => {
    try {
      // Use url-unshort to resolve the URL
      const expandedUrl = await uu.expand(url);

      if (expandedUrl) {
        return expandedUrl;
      } else {
        logger.info(`This URL can't be expanded: ${url}`);
        return url; // Return the original URL if it can't be expanded
      }
    } catch (error) {
      // Check if error is retryable based on specific substrings in error message
      const retryableErrors = ["ECONNRESET", "ETIMEDOUT"];
      if (retryableErrors.some((code) => error.message.includes(code))) {
        logger.error(`Retryable error occurred: ${error.message}`);
        throw error; // Rethrow error to trigger retry
      } else {
        logger.error(`Non-retryable error occurred: ${error.message}`);
        return url; // Skip retries and return original URL
      }
    }
  });
}

// Function to check if the URL should be skipped based on its extension (Blocklist: SKIPPED_EXTENSIONS)
// e.g. if a URL ends with .mp4 or .pdf file extension, it should be skipped
function skipUrlByFileExtension(url) {
  const lowercasedUrl = url.toLowerCase();
  const extPattern = new RegExp(
    `\\.(${Array.from(SKIPPED_EXTENSIONS)
      .map((ext) => ext.slice(1))
      .join("|")})$`,
    "i"
  );

  const isSkipped = extPattern.test(lowercasedUrl);

  if (isSkipped) {
    logger.info(`URL ${url} matches an extension to skip.`);
  }

  return isSkipped;
}

// Function to check if the URL should be skipped based on its host name (Blocklist: SKIPPED_HOSTS)
// e.g. if a URL is reddit.com, it should be skipped
function skipUrlByHost(url) {
  try {
    const parsedUrl = new URL(url); // Parse the URL to get the host
    const hostname = parsedUrl.hostname.toLowerCase();

    // Check if the hostname matches any of the predefined skipped hosts
    const isSkipped = SKIPPED_HOSTS.some(
      (host) => hostname === host || hostname.endsWith(`.${host}`)
    );

    if (isSkipped) {
      logger.info(`URL ${url} matches a host to skip: ${hostname}`);
    }

    return isSkipped;
  } catch (error) {
    logger.error(`Failed to parse URL ${url}: ${error.message}`);
    return false; // Return false if URL parsing fails
  }
}

// Function to fetch URL content with fallback to proxy if necessary
async function fetchWithFallback(url) {
  const headers = {
    "User-Agent":
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
  };

  try {
    // Stream the response data
    const response = await axios.get(url, {
      headers,
      responseType: "stream", // Use streaming
    });

    // Track the downloaded size
    let downloadedSize = 0;
    let aborted = false; // Flag to check if download is aborted

    const dataStream = new PassThrough();

    // Listen for 'data' events to track the size
    response.data.on("data", (chunk) => {
      downloadedSize += chunk.length;

      // Abort if the size exceeds the limit
      if (downloadedSize > config.botSettings.maxUrlSize) {
        logger.info(
          `Aborting download url ${url}: file too large (${downloadedSize} bytes)`
        );
        response.data.destroy(); // Abort the request
        aborted = true; // Mark as aborted
        dataStream.end(); // Close the stream to prevent further data
      } else {
        // Continue passing data if the size limit is not exceeded
        dataStream.write(chunk);
      }
    });

    // Handle the end of the stream
    response.data.on("end", () => {
      if (!aborted) {
        dataStream.end(); // Properly end the stream if not aborted
      }
    });

    // Handle errors
    response.data.on("error", (error) => {
      logger.error(`Error during download: ${error.message}`);
      dataStream.destroy(error); // Pass the error through the stream
    });

    // Collect the full data (if it doesn't exceed the size limit)
    let fullData = "";
    for await (const chunk of dataStream) {
      fullData += chunk.toString();
    }

    // Return empty content if the download was aborted due to size limit
    if (aborted) {
      return "";
    }

    return fullData;
  } catch (error) {
    logger.error(`GET request failed for URL ${url}: ${error.message}`);
    if (config.proxyOptions.fetchWithProxy) {
      return await fetchWithProxy(url, headers);
    }
    throw error;
  }
}

// Function to fetch URL with proxy support
export async function fetchWithProxy(url, headers) {
  const proxyUrl = `${config.proxyOptions.proxyBaseUrl}${encodeURIComponent(
    url
  )}`;

  try {
    // Stream the response data
    const response = await axios.get(proxyUrl, {
      headers,
      responseType: "stream", // Use streaming
    });

    // Track the downloaded size
    let downloadedSize = 0;
    let aborted = false; // Flag to check if download is aborted

    const dataStream = new PassThrough();

    // Listen for 'data' events to track the size
    response.data.on("data", (chunk) => {
      downloadedSize += chunk.length;

      // Abort if the size exceeds the limit
      if (downloadedSize > config.botSettings.maxUrlSize) {
        logger.info(
          `Aborting download url ${url} via proxy: file too large (${downloadedSize} bytes)`
        );
        response.data.destroy(); // Abort the request
        aborted = true; // Mark as aborted
        dataStream.end(); // Close the stream to prevent further data
      } else {
        // Continue passing data if the size limit is not exceeded
        dataStream.write(chunk);
      }
    });

    // Handle the end of the stream
    response.data.on("end", () => {
      if (!aborted) {
        dataStream.end(); // Properly end the stream if not aborted
      }
    });

    // Handle errors
    response.data.on("error", (error) => {
      logger.error(`Error during proxy download: ${error.message}`);
      dataStream.destroy(error); // Pass the error through the stream
    });

    // Collect the full data (if it doesn't exceed the size limit)
    let fullData = "";
    for await (const chunk of dataStream) {
      fullData += chunk.toString();
    }

    // Return empty content if the download was aborted due to size limit
    if (aborted) {
      return "";
    }

    return fullData;
  } catch (error) {
    logger.error(`Proxy GET request failed for URL ${url}: ${error.message}`);
    throw error;
  }
}

// Function to process HTML content
async function processHtml(responseData, url) {
  // primary context extender method
  let content = await processHtmlWithReadability(responseData, url);

  if (content) {
    return { content, summarizable: true, src: "primaryContextExtender" }; // Generated by Readability, summarizable is true
  }

  if (!content && config.botSettings.secondaryContextExtension === true) {
    // secondary context extender method (fallback method)
    content = await processHtmlWithCheerio(responseData);
    if (content) {
      return { content, summarizable: false, src: "secondaryContextExtender" }; // Generated by Cheerio, summarizable is false
    }
  }

  // If no content generated
  return { content: "", summarizable: false, src: null };
}
